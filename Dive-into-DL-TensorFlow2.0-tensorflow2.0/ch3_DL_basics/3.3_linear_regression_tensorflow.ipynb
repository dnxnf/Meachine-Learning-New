#%% md


# 线性回归的简洁实现

随着深度学习框架的发展，开发深度学习应用变得越来越便利。实践中，我们通常可以用比上一节更简洁的代码来实现同样的模型。在本节中，我们将介绍如何使用tensorflow2.0推荐的keras接口更方便地实现线性回归的训练。

## 生成数据集

我们生成与上一节中相同的数据集。其中`features`是训练数据特征，`labels`是标签。

#%%

import tensorflow as tf
num_inputs = 2
num_examples = 1000
true_w = [2, -3.4]
true_b = 4.2
features = tf.random.normal([num_examples, num_inputs],mean=0,stddev=1)
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels += tf.random.normal(labels.shape,0,1)
print(features[0], labels[0])

#%% md

虽然tensorflow2.0对于线性回归可以直接拟合，不用再划分数据集，但我们仍学习一下读取数据的方法

#%%

dataset=tf.data.Dataset.from_tensor_slices((features,labels))
train_db = tf.data.Dataset.from_tensor_slices((features, labels)).batch(10)
for(x,y) in train_db:
    print(x,y)

#%% md

定义模型,tensorflow 2.0推荐使用keras定义网络，故使用keras定义网络
我们先定义一个模型变量`model`，它是一个`Sequential`实例。
在keras中，`Sequential`实例可以看作是一个串联各个层的容器。
在构造模型时，我们在该容器中依次添加层。
当给定输入数据时，容器中的每一层将依次计算并将输出作为下一层的输入。
重要的一点是，在keras中我们无须指定每一层输入的形状。
因为为线性回归，输入层与输出层全连接，故定义一层

#%%

from tensorflow import keras
model = keras.Sequential()
model.add(keras.layers.Dense(1))

#%% md

定义损失函数和优化器：损失函数为mse，优化器选择sgd随机梯度下降
在keras中，定义完模型后，调用`compile()`方法可以配置模型的损失函数和优化方法。定义损失函数只需传入`loss`的参数，keras定义了各种损失函数，并直接使用它提供的平方损失`mse`作为模型的损失函数。同样，我们也无须实现小批量随机梯度下降，只需传入`optimizer`的参数，keras定义了各种优化算法，我们这里直接指定学习率为0.01的小批量随机梯度下降`tf.keras.optimizers.SGD(0.03)`为优化算法

#%%

model.compile(optimizer=tf.keras.optimizers.SGD(0.01),
              loss='mse')

#%% md

在使用keras训练模型时，我们通过调用`model`实例的`fit`函数来迭代模型。`fit`函数只需传入你的输入x和输出y，还有epoch遍历数据的次数，每次更新梯度的大小batch_size, 这里定义epoch=3，batch_size=10。
使用keras甚至完全不需要去划分数据集

#%%

model.fit(features,labels,epochs=3, batch_size=10)

#%% md

下面我们分别比较学到的模型参数和真实的模型参数。我们可以通过model的`get_weights()`来获得其权重（`weight`）和偏差（`bias`）。学到的参数和真实的参数很接近。

#%%

true_w, model.get_weights()[0]
true_w, model.get_weights()[0]
