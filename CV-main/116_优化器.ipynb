{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 优化器\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "① 损失函数调用 backward 方法，就可以调用损失函数的反向传播方法，就可以求出我们需要调节的梯度，我们就可以利用我们的优化器就可以根据梯度对参数进行调整，达到整体误差降低的目的。\n",
    "\n",
    "② 梯度要清零，如果梯度不清零会导致梯度累加。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 神经网络优化一轮\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2990, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3462, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2930, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2790, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3023, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2776, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2654, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2530, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2054, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1965, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1876, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2011, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2157, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2147, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1963, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1902, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2693, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1461, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0569, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1459, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1481, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0632, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0564, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.1163, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9630, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9376, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9896, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0338, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8928, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0101, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9139, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9293, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9084, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0023, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8759, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0181, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9389, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7936, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9860, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9058, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0109, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7828, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0021, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9517, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0191, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9020, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8756, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9103, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7571, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0279, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0523, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8963, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9765, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8693, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9491, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0137, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8450, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8853, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8180, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8033, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6458, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8837, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9848, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8718, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6739, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8396, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7901, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7137, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8315, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7396, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8922, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8063, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7126, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8943, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5841, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7079, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6653, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7751, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6968, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6305, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8891, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7878, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9873, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7357, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7106, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7518, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7236, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6429, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5970, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6645, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8054, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6327, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6302, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6355, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7187, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7549, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7519, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5339, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6832, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7757, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5539, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7727, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6997, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5982, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5259, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7740, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8887, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8386, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6146, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6335, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6341, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7324, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5724, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5783, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6211, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6137, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5161, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6996, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7001, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.3892, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4379, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6847, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8142, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6243, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5475, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7929, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6979, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6352, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7864, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7133, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6351, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8596, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7043, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5428, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6088, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5955, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4176, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7358, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6282, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6812, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6691, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5881, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5164, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5346, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6331, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5344, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7009, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.6702, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4784, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4571, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4409, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7151, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4511, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.7351, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.5353, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\n",
    "    \"./dataset\", train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=64, drop_last=True)\n",
    "\n",
    "\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()\n",
    "        self.model1 = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "loss = nn.CrossEntropyLoss()  # 交叉熵\n",
    "# loss = nn.MSELoss\n",
    "tudui = Tudui()\n",
    "# optim = torch.optim.SGD(tudui.parameters(),lr=0.01)   # 随机梯度下降优化器\n",
    "optim = torch.optim.Adam(tudui.parameters(), lr=0.001)  # 随机梯度下降优化器\n",
    "# 计算损失，梯度清零，反向传播，参数更新\n",
    "for data in dataloader:\n",
    "    imgs, targets = data\n",
    "    outputs = tudui(imgs)\n",
    "    result_loss = loss(outputs, targets)  # 计算实际输出与目标输出的差距\n",
    "    optim.zero_grad()  # 梯度清零\n",
    "    result_loss.backward()  # 反向传播，计算损失函数的梯度\n",
    "    optim.step()  # 根据梯度，对网络的参数进行调优\n",
    "    print(result_loss)  # 对数据只看了一遍，只看了一轮，所以loss下降不大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 神经网络优化多轮\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(285.9961, grad_fn=<AddBackward0>)\n",
      "tensor(231.3197, grad_fn=<AddBackward0>)\n",
      "tensor(204.8078, grad_fn=<AddBackward0>)\n",
      "tensor(182.5047, grad_fn=<AddBackward0>)\n",
      "tensor(163.2892, grad_fn=<AddBackward0>)\n",
      "tensor(146.5211, grad_fn=<AddBackward0>)\n",
      "tensor(127.8201, grad_fn=<AddBackward0>)\n",
      "tensor(110.0710, grad_fn=<AddBackward0>)\n",
      "tensor(95.1598, grad_fn=<AddBackward0>)\n",
      "tensor(85.7083, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\n",
    "    \"./dataset\", train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=64, drop_last=True)\n",
    "\n",
    "\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()\n",
    "        self.model1 = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "loss = nn.CrossEntropyLoss()  # 交叉熵\n",
    "tudui = Tudui()\n",
    "# optim = torch.optim.SGD(tudui.parameters(),lr=0.01)   # 随机梯度下降优化器\n",
    "#\n",
    "optim = torch.optim.Adam(tudui.parameters(), lr=0.001)  # 动量优化器,adam已经内置了动量\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for data in dataloader:\n",
    "        imgs, targets = data\n",
    "        outputs = tudui(imgs)\n",
    "        result_loss = loss(outputs, targets)  # 计算实际输出与目标输出的差距\n",
    "        optim.zero_grad()  # 梯度清零\n",
    "        result_loss.backward()  # 反向传播，计算损失函数的梯度\n",
    "        optim.step()  # 根据梯度，对网络的参数进行调优\n",
    "        running_loss = running_loss + result_loss\n",
    "    print(running_loss)  # 对这一轮所有误差的总和"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 神经网络学习率优化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset len:  156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\py\\Anaconda3\\envs\\openDS\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:182: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m result_loss \u001b[38;5;241m=\u001b[39m loss(outputs, targets)  \u001b[38;5;66;03m# 计算实际输出与目标输出的差距\u001b[39;00m\n\u001b[0;32m     49\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# 梯度清零\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mresult_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 反向传播，计算损失函数的梯度\u001b[39;00m\n\u001b[0;32m     51\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# 根据梯度，对网络的参数进行调优\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# 学习率太小了，所以20个轮次后，相当于没走多少\u001b[39;00m\n",
      "File \u001b[1;32md:\\py\\Anaconda3\\envs\\openDS\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\py\\Anaconda3\\envs\\openDS\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\py\\Anaconda3\\envs\\openDS\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequential\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(\n",
    "    \"./dataset\", train=False, transform=torchvision.transforms.ToTensor(), download=True\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=64, drop_last=True)\n",
    "\n",
    "\n",
    "class Tudui(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Tudui, self).__init__()\n",
    "        self.model1 = Sequential(\n",
    "            Conv2d(3, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 32, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Conv2d(32, 64, 5, padding=2),\n",
    "            MaxPool2d(2),\n",
    "            Flatten(),\n",
    "            Linear(1024, 64),\n",
    "            Linear(64, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "loss = nn.CrossEntropyLoss()  # 交叉熵\n",
    "tudui = Tudui()\n",
    "# optim = torch.optim.SGD(tudui.parameters(), lr=0.01)  # 随机梯度下降优化器\n",
    "optim = torch.optim.Adam(tudui.parameters(), lr=0.001)  # 动量优化器\n",
    "\n",
    "# todo 学习率衰减,每过 step_size 更新一次优化器，更新是学习率为原来的学习率的的 0.1 倍\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.1)\n",
    "print(\"dataset len: \",len(dataloader))\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    scheduler.step()#  学习率衰减,放在每轮epoch之后，如果在内层，则很快就收敛\n",
    "    for data in dataloader:\n",
    "        imgs, targets = data\n",
    "        outputs = tudui(imgs)\n",
    "        result_loss = loss(outputs, targets)  # 计算实际输出与目标输出的差距\n",
    "        optim.zero_grad()  # 梯度清零\n",
    "        result_loss.backward()  # 反向传播，计算损失函数的梯度\n",
    "        optim.step()  # 根据梯度，对网络的参数进行调优\n",
    "        \n",
    "        # 学习率太小了，所以20个轮次后，相当于没走多少\n",
    "        running_loss = running_loss + result_loss\n",
    "    print(running_loss)  # 对这一轮所有误差的总和"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch310_jupyter",
   "language": "python",
   "name": "pytorch310_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "350px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
