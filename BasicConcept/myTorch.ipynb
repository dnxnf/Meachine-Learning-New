{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca66668",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dataset和DataLoader，以及训练参数。](#toc1_1_)    \n",
    "    - [epoch，batch_size等参数](#toc1_1_1_)    \n",
    "  - [torchvision](#toc1_2_)    \n",
    "  - [学习率和优化器：](#toc1_3_)    \n",
    "    - [学习率](#toc1_3_1_)    \n",
    "    - [优化器](#toc1_3_2_)    \n",
    "  - [损失函数](#toc1_4_)    \n",
    "  - [卷积](#toc1_5_)    \n",
    "  - [池化层](#toc1_6_)    \n",
    "    - [最大池化](#toc1_6_1_)    \n",
    "    - [平均池化](#toc1_6_2_)    \n",
    "    - [反池化](#toc1_6_3_)    \n",
    "  - [线性层](#toc1_7_)    \n",
    "  - [激活函数层](#toc1_8_)    \n",
    "  - [权重初始化](#toc1_9_)    \n",
    "    - [激活函数与推荐初始化方法](#toc1_9_1_)    \n",
    "  - [Hook 函数](#toc1_10_)    \n",
    "  - [正则化](#toc1_11_)    \n",
    "  - [归一化](#toc1_12_)    \n",
    "  - [模型保存与加载](#toc1_13_)    \n",
    "    - [基本方法](#toc1_13_1_)    \n",
    "      - [(1) 保存整个模型（结构+参数）](#toc1_13_1_1_)    \n",
    "      - [(2) 仅保存模型参数（推荐）](#toc1_13_1_2_)    \n",
    "    - [带优化器的保存（用于恢复训练）](#toc1_13_2_)    \n",
    "    - [跨设备加载](#toc1_13_3_)    \n",
    "    - [注意事项](#toc1_13_4_)    \n",
    "    - [完整示例代码](#toc1_13_5_)    \n",
    "    - [常用文件结构](#toc1_13_6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e50a13",
   "metadata": {},
   "source": [
    "我的 PyTorch 笔记和代码\n",
    "\n",
    "## <a id='toc1_1_'></a>[Dataset和DataLoader，以及训练参数。](#toc0_)\n",
    "\n",
    "Dataset: 自定义数据集，包括读取数据集、处理方式等\n",
    "DataLoader: 加载数据集，将数据集装载为一个可迭代对象\n",
    "Model: 定义模型，包括网络结构、参数等\n",
    "Criterion: 定义损失函数，包括计算损失的公式\n",
    "Criterion: 定义损失函数\n",
    "Optimizer: 定义优化器\n",
    "Trainer: 训练模型\n",
    "\n",
    "### <a id='toc1_1_1_'></a>[epoch，batch_size等参数](#toc0_)\n",
    "Epoch: 所有训练样本都已经输入到模型中，称为一个 Epoch\n",
    "epoch 指定所有样本被训练几轮，决定了模型将看到整个数据集多少次，常用 50,100，太多会过拟合\n",
    "\n",
    "Iteration: 一批样本输入到模型中，称为一个 Iteration\n",
    "即迭代次数，完整训练一次所有样本需要的次数\n",
    "\n",
    "Batchsize: 批大小，决定一个 iteration 有多少样本，也决定了一个 Epoch 有多少个 Iteration。\n",
    "\n",
    "每次输入到模型上的样本数，一般为 16、32、64、128、256、512 等，太多了会爆显存，根据模型而已，有的 64 就会爆，修妖减小才行，大的 batchsize 训练更稳定，训练时间长，收敛效果好，小的 batchsize 训练更快，训练时间短，收敛效果差（噪声大）。\n",
    "\n",
    "Iteration \\* Batchsize = 一轮的样本总数，即一个 epoch 要训练多少个样本\n",
    "Total Iterations = Number of Epochs × (Total Samples / Batch Size)\n",
    "总轮次 = 总 Epoch 数 × (样本总数 / 批大小)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19362a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型典型流程\n",
    "for epoch in range(num_epochs):  # 外层循环控制Epoch\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # 内层循环处理Iteration\n",
    "        # 1. 将数据移动到设备（GPU/CPU）\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # 2. 前向传播\n",
    "        output = model(data)\n",
    "\n",
    "        # 3. 计算损失\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # 4. 反向传播\n",
    "        optimizer.zero_grad()  # 清空梯度\n",
    "        loss.backward()  # 计算梯度\n",
    "\n",
    "        # 5. 参数更新\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f3ef3",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[torchvision](#toc0_)\n",
    "\n",
    "我们在安装 PyTorch 时，还安装了 torchvision，这是一个计算机视觉工具包。有 3 个主要的模块：\n",
    "torchvision.transforms 模块提供了一些常用的图像预处理方法，如裁剪、缩放、旋转、翻转等。\n",
    "torchvision.datasets: 里面包括常用数据集如 mnist、CIFAR-10、Image-Net 等\n",
    "torchvision.models: 里面包括常用的模型如 AlexNet、VGG、ResNet、GoogleNet 等。\n",
    "\n",
    "ps:torchvision 是计算机视觉库，提供常用的数据集和预处理方法\n",
    "\n",
    "PyTorch 的主要模块有：\n",
    "torch：核心模块，包含张量计算、自动求导、并行计算等功能。\n",
    "torch.nn：神经网络模块，包含各种神经网络层、损失函数等。\n",
    "torch.optim：优化器模块，包含常用的优化算法。\n",
    "torch.utils.data：数据集模块，包含常用的数据集加载器。\n",
    "torch.autograd：自动求导模块，包含自动求导引擎。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ac4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 设置训练集的数据增强和转化\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32, 32)),  # 缩放\n",
    "        transforms.RandomCrop(32, padding=4),  # 裁剪\n",
    "        transforms.ToTensor(),  # 转为张量，同时归一化\n",
    "        transforms.Normalize(norm_mean, norm_std),  # 标准化\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 设置验证集的数据增强和转化，不需要 RandomCrop\n",
    "valid_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(norm_mean, norm_std),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ab4427",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[学习率和优化器：](#toc0_)\n",
    "\n",
    "### <a id='toc1_3_1_'></a>[学习率](#toc0_)\n",
    "\n",
    "LR，控制每次参数更新的步长，是优化器调整模型权重的幅度，一般要前打后小，并且总体不能过大，否则会导致模型震荡。\n",
    "\n",
    "常使用学习率衰减策略，包括余弦衰减、指数衰减、线性衰减等。\n",
    "\n",
    "### <a id='toc1_3_2_'></a>[优化器](#toc0_)\n",
    "\n",
    "优化器是指模型训练时使用的算法，包括SGD、Adam、Adagrad、Adadelta、RMSprop等。\n",
    "\n",
    "SGD：随机梯度下降，是最常用的优化器，其基本思想是每次迭代时，随机选择一个样本，计算其梯度，然后更新模型参数。\n",
    "\n",
    "Adam：Adam是一种基于动量的优化器，其基本思想是计算梯度的指数加权移动平均值，使得模型的更新方向更加准确。也是默认学习器，大多数时候已经足够好\n",
    "（Adam的优点是能够自适应调整学习率，并且内置了动量，不能再手动指定动量）\n",
    "\n",
    "AdamW：AdamW是Adam的变体，其在Adam的基础上，增加了权重衰减项，可以防止过拟合。\n",
    "\n",
    "RMPprop：RMSprop是Adadelta的变体，其基本思想是对梯度的指数加权移动平均值，并对其进行平方根处理，使得更新更加稳定。用于RNN和LSTM。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1668985",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[损失函数](#toc0_)\n",
    "\n",
    "MSE：均方误差损失，即L2损失函数，即预测值与真实值之差的平方的平均值，常用于回归问题，预测的是连续值。\n",
    "$$MSE=\\frac{1}{n}\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}$$\n",
    "\n",
    "MAE：平均绝对误差，即L1损失函数，即预测值与真实值之差的绝对值的平均值，常用于回归问题，预测的是连续值。\n",
    "$$MAE=\\frac{1}{n}\\sum_{i=1}^{n}\\left|y_{i}-\\hat{y}_{i}\\right|$$\n",
    "\n",
    "CrossEntropyLoss：交叉熵损失函数：分类问题常用的损失函数，衡量两个概率分布之间的差异。\n",
    "$$H(p,q)=-\\sum_{i=1}^{n}p_{i}\\log(q_{i})$$\n",
    "\n",
    "其中，$p$和$q$分别是真实分布和预测分布，$n$是样本数。\n",
    "\n",
    "CrossEntropyLoss和Softmax函数的结合：\n",
    "\n",
    "CrossEntropyLoss函数的输入是预测的概率分布，而Softmax函数的输入是预测的类别分布。因此，我们可以将CrossEntropyLoss函数的输入经过Softmax函数转换为类别分布，从而得到最终的损失值。\n",
    "\n",
    "$$\\ell(x,y)=\\frac{1}{n}\\sum_{i=1}^{n}\\log\\left(\\frac{\\exp(x_{i,y})}{\\sum_{j=1}^{k}\\exp(x_{i,j})}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1773b3f5",
   "metadata": {},
   "source": [
    "创建一个模型\n",
    "\n",
    "1.putorch中，模型的定义一般是继承nn.Module类，并实现__init__()方法和forward()方法。\n",
    "2.在__init__()方法中，定义模型的层，例如卷积层、全连接层等。\n",
    "3.在forward()方法中，对层进行拼接，实现模型的前向传播过程，即输入数据经过模型得到输出。\n",
    "\n",
    "在拼接层时，可以用nn.Sequential()函数，它可以将多个层组合成一个模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9f8ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义一个模型\n",
    "class MyTorch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyTorch, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230affd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=3, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=3, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[-0.0501]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 定义一个加入了nn.sequential的简单模型\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "net = MyModel()\n",
    "print(net)\n",
    "net = net(torch.randn(1, 2)) # 输入1个样本，2个特征\n",
    "# net = net(torch.randn(2, 2)) # 输入2个样本，2个特征\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c9369c",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[卷积](#toc0_)\n",
    "\n",
    "二维卷积：Conv2d，\n",
    "```python\n",
    "nn.Conv2d(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1,\n",
    "                 bias=True, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "三维卷积：Conv3d，\n",
    "```python\n",
    "nn.Conv3d(self, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1,\n",
    "                 bias=True, padding_mode='zeros')\n",
    "```\n",
    "包括输入通道数，输出通道数，卷积核大小，步长，填充，膨胀，组数，偏置，填充模式。\n",
    "一般就指定前五个，输入，输出，卷积核大小，填充（0），步长（1），后面都是默认\n",
    "步长为1是卷积，步长为2是下采样\n",
    "参数说明：\n",
    "in_channels\t输入数据的通道数（如RGB图像=3，灰度图=1）\n",
    "\n",
    "out_channels\t卷积后输出的通道数（即卷积核的数量）\n",
    "\n",
    "kernel_size\t卷积核的尺寸（整数或元组，如 3 或 (3,5)）\n",
    "\n",
    "stride\t卷积核移动的步长（控制输出尺寸的缩小比例）\n",
    "\n",
    "padding\t输入边缘填充的像素数（保持空间分辨率）\n",
    "\n",
    "dilation\t卷积核的膨胀率（扩大感受野，如空洞卷积）\n",
    "\n",
    "groups\t分组卷积的组数（in_channels和out_channels需能被整除）\n",
    "\n",
    "bias\t是否添加可学习的偏置项（True/False）\n",
    "\n",
    "padding_mode\t填充方式（'zeros'、'reflect'、'replicate'）\n",
    "\n",
    "这里不考虑空洞卷积，假设输入图片大小为 $ I \\times I$，卷积核大小为 $k \\times k$，stride 为 $s$，padding 的像素数为 $p$，图片经过卷积之后的尺寸 $ O $ 如下：\n",
    "\n",
    "$O = \\displaystyle\\frac{I -k + 2 \\times p}{s} +1$\n",
    "\n",
    "下面例子的输入图片大小为 $5 \\times 5$，卷积大小为 $3 \\times 3$，stride 为 1，padding 为 0，所以输出图片大小为 $\\displaystyle\\frac{5 -3 + 2 \\times 0}{1} +1 = 3$。\n",
    "\n",
    "因为padding是填两层，左右上下两个方向都填充了0。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83f4c77",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[池化层](#toc0_)\n",
    "\n",
    "池化的作用则体现在降采样：保留显著特征、降低特征维度，增大kernel的感受野。 \n",
    "\n",
    "另外一点值得注意：pooling也可以提供一些旋转不变性。 池化层可对提取到的特征信息进行降维，一方面使特征图变小，简化网络计算复杂度并在一定程度上避免过拟合的出现；一方面进行特征压缩，提取主要特征。\n",
    "\n",
    "有最大池化和平均池化两张方式。\n",
    "\n",
    "### <a id='toc1_6_1_'></a>[最大池化](#toc0_)\n",
    "\n",
    "nn.MaxPool2d()\n",
    "\n",
    "nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "这个函数的功能是进行 2 维的最大池化，主要参数如下：\n",
    "\n",
    "kernel_size：池化核尺寸\n",
    "\n",
    "stride：步长，通常与 kernel_size 一致\n",
    "\n",
    "padding：填充宽度，主要是为了调整输出的特征图大小，一般把 padding 设置合适的值后，保持输入和输出的图像尺寸不变。\n",
    "\n",
    "dilation：池化间隔大小，默认为1。常用于图像分割任务中，主要是为了提升感受野\n",
    "\n",
    "ceil_mode：默认为 False，尺寸向下取整。为 True 时，尺寸向上取整\n",
    "\n",
    "return_indices：为 True 时，返回最大池化所使用的像素的索引，这些记录的索引通常在反最大池化时使用，把小的特征图反池化到大的特征图时，每一个像素放在哪个位置。\n",
    "\n",
    "\n",
    "\n",
    "下面是最大池化的代码：\n",
    "\n",
    "```python\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from common_tools import transform_invert, set_seed\n",
    "\n",
    "set_seed(1)  # 设置随机种子\n",
    "\n",
    "# ================================= load img ==================================\n",
    "path_img = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"imgs/lena.png\")\n",
    "img = Image.open(path_img).convert('RGB')  # 0~255\n",
    "\n",
    "# convert to tensor\n",
    "img_transform = transforms.Compose([transforms.ToTensor()])\n",
    "img_tensor = img_transform(img)\n",
    "img_tensor.unsqueeze_(dim=0)    # C*H*W to B*C*H*W\n",
    "\n",
    "# ============= create convolution layer ===============\n",
    "\n",
    "# ================ maxpool\n",
    "flag = 1\n",
    "# flag = 0\n",
    "if flag:\n",
    "    maxpool_layer = nn.MaxPool2d((2, 2), stride=(2, 2))   # input:(i, o, size) weights:(o, i , h, w)\n",
    "    img_pool = maxpool_layer(img_tensor)\n",
    "\n",
    "print(\"池化前尺寸:{}\\n池化后尺寸:{}\".format(img_tensor.shape, img_pool.shape))\n",
    "img_pool = transform_invert(img_pool[0, 0:3, ...], img_transform)\n",
    "img_raw = transform_invert(img_tensor.squeeze(), img_transform)\n",
    "plt.subplot(122).imshow(img_pool)\n",
    "plt.subplot(121).imshow(img_raw)\n",
    "plt.show()\n",
    "\n",
    "池化前尺寸:torch.Size([1, 3, 512, 512])\n",
    "池化后尺寸:torch.Size([1, 3, 256, 256])\n",
    "```\n",
    "### <a id='toc1_6_2_'></a>[平均池化](#toc0_)\n",
    "\n",
    "```python\n",
    "nn.AvgPool2d()\n",
    "\n",
    "torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)\n",
    "这个函数的功能是进行 2 维的平均池化，主要参数如下：\n",
    "\n",
    "kernel_size：池化核尺寸\n",
    "\n",
    "stride：步长，通常与 kernel_size 一致\n",
    "\n",
    "padding：填充宽度，主要是为了调整输出的特征图大小，一般把 padding 设置合适的值后，保持输入和输出的图像尺寸不变。\n",
    "\n",
    "dilation：池化间隔大小，默认为1。常用于图像分割任务中，主要是为了提升感受野\n",
    "\n",
    "ceil_mode：默认为 False，尺寸向下取整。为 True 时，尺寸向上取整\n",
    "\n",
    "count_include_pad：在计算平均值时，是否把填充值考虑在内计算\n",
    "\n",
    "divisor_override：除法因子。在计算平均值时，分子是像素值的总和，分母默认是像素值的个数。如果设置了 divisor_override，把分母改为 divisor_override。\n",
    "\n",
    "img_tensor = torch.ones((1, 1, 4, 4))\n",
    "avgpool_layer = nn.AvgPool2d((2, 2), stride=(2, 2))\n",
    "img_pool = avgpool_layer(img_tensor)\n",
    "print(\"raw_img:\\n{}\\npooling_img:\\n{}\".format(img_tensor, img_pool))\n",
    "输出如下：\n",
    "\n",
    "raw_img:\n",
    "tensor([[[[1., 1., 1., 1.],\n",
    "          [1., 1., 1., 1.],\n",
    "          [1., 1., 1., 1.],\n",
    "          [1., 1., 1., 1.]]]])\n",
    "pooling_img:\n",
    "tensor([[[[1., 1.],\n",
    "          [1., 1.]]]])\n",
    "加上divisor_override=3后，输出如下：\n",
    "\n",
    "raw_img:\n",
    "tensor([[[[1., 1., 1., 1.],\n",
    "          [1., 1., 1., 1.],\n",
    "          [1., 1., 1., 1.],\n",
    "          [1., 1., 1., 1.]]]])\n",
    "pooling_img:\n",
    "tensor([[[[1.3333, 1.3333],\n",
    "          [1.3333, 1.3333]]]])\n",
    "nn.MaxUnpool2d()\n",
    "\n",
    "```\n",
    "\n",
    "### <a id='toc1_6_3_'></a>[反池化](#toc0_)\n",
    "\n",
    "```python\n",
    "nn.MaxUnpool2d(kernel_size, stride=None, padding=0)\n",
    "功能是对二维信号（图像）进行最大值反池化，主要参数如下：\n",
    "\n",
    "kernel_size：池化核尺寸\n",
    "\n",
    "stride：步长，通常与 kernel_size 一致\n",
    "\n",
    "padding：填充宽度\n",
    "\n",
    "代码如下：\n",
    "\n",
    "# pooling\n",
    "img_tensor = torch.randint(high=5, size=(1, 1, 4, 4), dtype=torch.float)\n",
    "maxpool_layer = nn.MaxPool2d((2, 2), stride=(2, 2), return_indices=True)\n",
    "img_pool, indices = maxpool_layer(img_tensor)\n",
    "\n",
    "# unpooling\n",
    "img_reconstruct = torch.randn_like(img_pool, dtype=torch.float)\n",
    "maxunpool_layer = nn.MaxUnpool2d((2, 2), stride=(2, 2))\n",
    "img_unpool = maxunpool_layer(img_reconstruct, indices)\n",
    "\n",
    "print(\"raw_img:\\n{}\\nimg_pool:\\n{}\".format(img_tensor, img_pool))\n",
    "print(\"img_reconstruct:\\n{}\\nimg_unpool:\\n{}\".format(img_reconstruct, img_unpool))\n",
    "输出如下：\n",
    "\n",
    "# pooling\n",
    "img_tensor = torch.randint(high=5, size=(1, 1, 4, 4), dtype=torch.float)\n",
    "maxpool_layer = nn.MaxPool2d((2, 2), stride=(2, 2), return_indices=True)\n",
    "img_pool, indices = maxpool_layer(img_tensor)\n",
    "\n",
    "# unpooling\n",
    "img_reconstruct = torch.randn_like(img_pool, dtype=torch.float)\n",
    "maxunpool_layer = nn.MaxUnpool2d((2, 2), stride=(2, 2))\n",
    "img_unpool = maxunpool_layer(img_reconstruct, indices)\n",
    "\n",
    "print(\"raw_img:\\n{}\\nimg_pool:\\n{}\".format(img_tensor, img_pool))\n",
    "print(\"img_reconstruct:\\n{}\\nimg_unpool:\\n{}\".format(img_reconstruct, img_unpool))\n",
    "\n",
    "```\n",
    "\n",
    "## <a id='toc1_7_'></a>[线性层](#toc0_)\n",
    "\n",
    "\n",
    "线性层又称为全连接层，其每个神经元与上一个层所有神经元相连，实现对前一层的线性组合或线性变换。其要求输入是一位向量，因此往往会将多维数据展开成一维的，因此会丢失空间结构。加上无法聚焦局部区域，空间结构混乱，一张图从左往右和从右往左会得到不同的向量表示，但是卷积层会有权值共享维持平移不变形。其y=Wx+b的形式，其中W是权重矩阵，b是偏置项。\n",
    "\n",
    "```python\n",
    "\n",
    "nn.Linear(in_features, out_features, bias=True) # NOTE 这个居然就是全连接层，2025.6.29,第一次知道\n",
    "\n",
    "代码如下：\n",
    "\n",
    "inputs = torch.tensor([[1., 2, 3]])\n",
    "linear_layer = nn.Linear(3, 4)\n",
    "linear_layer.weight.data = torch.tensor([[1., 1., 1.],\n",
    "[2., 2., 2.],\n",
    "[3., 3., 3.],\n",
    "[4., 4., 4.]])\n",
    "\n",
    "linear_layer.bias.data.fill_(0.5)\n",
    "output = linear_layer(inputs)\n",
    "print(inputs, inputs.shape)\n",
    "print(linear_layer.weight.data, linear_layer.weight.data.shape)\n",
    "print(output, output.shape)\n",
    "输出为：\n",
    "\n",
    "复制\n",
    "tensor([[1., 2., 3.]]) torch.Size([1, 3])\n",
    "tensor([[1., 1., 1.],\n",
    "        [2., 2., 2.],\n",
    "        [3., 3., 3.],\n",
    "        [4., 4., 4.]]) torch.Size([4, 3])\n",
    "tensor([[ 6.5000, 12.5000, 18.5000, 24.5000]], grad_fn=<AddmmBackward>) torch.Size([1, 4])\n",
    "\n",
    "```\n",
    "\n",
    "## <a id='toc1_8_'></a>[激活函数层](#toc0_)\n",
    "\n",
    "激活函数层的作用是对神经网络的输出进行非线性变换，使得输出值不再是线性的，从而使得神经网络能够拟合非线性数据。\n",
    "假设第一个隐藏层为：$H_{1}=X \\times W_{1}$，第二个隐藏层为：$H_{2}=H_{1} \\times W_{2}$，输出层为：\n",
    "\n",
    "$ \\begin{aligned} \\text { Out } \\boldsymbol{p} \\boldsymbol{u} \\boldsymbol{t} &=\\boldsymbol{H}{2} * \\boldsymbol{W}{3} \\ &=\\boldsymbol{H}{1} * \\boldsymbol{W}{2} \\boldsymbol{W}_{3} \\ &=\\boldsymbol{X} (\\boldsymbol{W}{1} *\\boldsymbol{W}{2} \\boldsymbol{W}_{3}) \\ &=\\boldsymbol{X} {W} \\end{aligned} $\n",
    "\n",
    "如果没有非线性变换，由于矩阵乘法的结合性，多个线性层的组合等价于一个线性层。\n",
    "\n",
    "激活函数对特征进行非线性变换，赋予了多层神经网络具有深度的意义。下面介绍一些激活函数层。\n",
    "\n",
    "nn.Sigmoid\n",
    "计算公式：$y=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "梯度公式：$y^{\\prime}=y *(1-y)$\n",
    "\n",
    "特性：\n",
    "\n",
    "输出值在(0,1)，符合概率\n",
    "\n",
    "导数范围是 [0, 0.25]，容易导致梯度消失\n",
    "\n",
    "输出为非 0 均值，破坏数据分布\n",
    "\n",
    "nn.tanh\n",
    "计算公式：$y=\\frac{\\sin x}{\\cos x}=\\frac{e^{x}-e^{-x}}{e^{-}+e^{-x}}=\\frac{2}{1+e^{-2 x}}+1$\n",
    "\n",
    "梯度公式：$y^{\\prime}=1-y^{2}$\n",
    "\n",
    "特性：\n",
    "\n",
    "输出值在(-1, 1)，数据符合 0 均值\n",
    "\n",
    "导数范围是 (0,1)，容易导致梯度消失\n",
    "\n",
    "nn.ReLU(修正线性单元)\n",
    "计算公式：$y=max(0, x)$\n",
    "\n",
    "梯度公式：$y' = \\begin{cases} 1 & x > 0 \\\\ 0 & x < 0 \\\\ \\text{undefined} & x=0 \\end{cases}$\n",
    "\n",
    "特性：\n",
    "\n",
    "输出值均为正数，负半轴的导数为 0，容易导致死神经元\n",
    "\n",
    "导数是 1，缓解梯度消失，但容易引发梯度爆炸\n",
    "\n",
    "\n",
    "针对 RuLU 会导致死神经元的缺点，出现了下面 3 种改进的激活函数。\n",
    "\n",
    "nn.LeakyReLU\n",
    "有一个参数negative_slope：设置负半轴斜率\n",
    "\n",
    "nn.PReLU\n",
    "有一个参数init：设置初始斜率，这个斜率是可学习的\n",
    "\n",
    "nn.RReLU\n",
    "R 是 random 的意思，负半轴每次斜率都是随机取 [lower, upper] 之间的一个数\n",
    "\n",
    "lower：均匀分布下限\n",
    "\n",
    "upper：均匀分布上限"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf56519",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[权重初始化](#toc0_)\n",
    "\n",
    "全0（不好）：所有权重都设置为0\n",
    "\n",
    "随机初始化：权重服从均值为0，标准差为1的正态分布，如果初始值过大或过小，可能导致梯度爆炸或消失。\n",
    "\n",
    "Xavier初始化：权重服从均值为0，标准差为2/n的正态分布，其中n是输入维度,通过修改标准差，让\n",
    "\n",
    "He初始化：权重服从均值为0，标准差为2/sqrt(n)的正态分布，其中n是输入维度。针对ReLU激活函数效果更好。\n",
    "\n",
    "He初始化和Xavier初始化都会向内挤压高斯分布，高斯分布有更加尖锐的峰值，其最高点要大得多。，这样的神经元更不容易饱和。\n",
    "\n",
    "见神经网络与机器学习P86，里面写的：假设我们有⼀个有 nin 个输⼊权重的神经元。我们会使⽤均值为 0 标准差为 1/√nin 的高斯随机分布初始化这些权重。也就是说，我们会向下挤压⾼斯分布，让我们的神经元更不可能饱和\n",
    "\n",
    "### <a id='toc1_9_1_'></a>[激活函数与推荐初始化方法](#toc0_)\n",
    "\n",
    "激活函数\t            推荐初始化方法\n",
    "\n",
    "Sigmoid / Tanh\t        Xavier/Glorot\n",
    "\n",
    "ReLU / LeakyReLU\t    He/Kaiming\n",
    "\n",
    "默认用 Xavier（Sigmoid/Tanh）或 He（ReLU）初始化，避免全零初始化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded5c86",
   "metadata": {},
   "source": [
    "## <a id='toc1_10_'></a>[Hook 函数](#toc0_)\n",
    "\n",
    "是在不改变主体的情况下，实现额外功能。由于 PyTorch 是基于动态图实现的，因此在一次迭代运算结束后，一些中间变量如非叶子节点的梯度和特征图，会被释放掉。在这种情况下想要提取和记录这些中间变量，就需要使用 Hook 函数。（比如之前在bcp中提取中间的特征图）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb6612d",
   "metadata": {},
   "source": [
    "## <a id='toc1_11_'></a>[正则化](#toc0_)\n",
    "\n",
    "L1，L2正则化，本质都是缩小权重，但是L1正则化对小权重的效果更明显，对大权重的效果弱于L2正则化，L2正则化对大权重的效果更明显。因此表现为L1让权重变得稀疏，L2让权重变得更加平滑。 也就是删小减大。具体可以见神经网络与机器学习P79。\n",
    "\n",
    "L1正则化：给损失函数加上函数的绝对值之和，即：loss = loss + alpha * sum(abs(w))\n",
    "\n",
    "$\\mathcal{L} = \\mathcal{L} + \\alpha \\cdot \\sum |w|$\n",
    "\n",
    "更规范的写法是\n",
    "\n",
    "$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{original}} + \\alpha \\cdot \\|\\mathbf{w}\\|_1$\n",
    "\n",
    "L2正则化：给损失函数加上函数的平方和，即：loss = loss + alpha * sum(w^2)\n",
    "\n",
    "$\\mathcal{L} = \\mathcal{L} + \\alpha \\cdot \\sum w^2$\n",
    "\n",
    "更规范的写法是\n",
    "\n",
    "$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{original}} + \\alpha \\cdot \\|\\mathbf{w}\\|_2^2$\n",
    "\n",
    "Dropout：随机让某些神经元不工作，让网络不要太过依赖于某个神经元，变相降低权重，防止过拟合，公式为：\n",
    "\n",
    "$\\mathcal{L} = \\mathcal{L} + \\text{dropout}(p) \\cdot \\sum_{i=1}^n \\frac{1}{2}(1-p) \\cdot \\sigma(z_i)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2ee714",
   "metadata": {},
   "source": [
    "## <a id='toc1_12_'></a>[归一化](#toc0_)\n",
    "\n",
    "常用且效果好的就是GN和BN，为组归一化和批归一化。归一化指的是将数据映射到[0,1]或[-1,1]的范围内，使得数据分布变得更加均匀，从而加速训练，也能使模型训练更加稳定。\n",
    "\n",
    "GN：Group Normalization，组归一化，将输入分组，每组内的数据归一化，然后再将各组数据拼接起来，GN的思想是对输入进行分组，每组数据归一化，然后再拼接。其不依赖batch维度，可以适用于batchsize=1的任务，如检测和分割。公式为：\n",
    "\n",
    "$$ \\hat{x}_i =\\gamma \\frac{x_i - \\mu_\\mathcal{G}}{\\sqrt{\\sigma_\\mathcal{G}^2 + \\epsilon}} + \\beta $$\n",
    "\n",
    "其中$\\hat{x}_i$是归一化后的输入，$\\mu_\\mathcal{G}$和$\\sigma_\\mathcal{G}^2$是组内均值和方差，$\\gamma$和$\\beta$是可学习的系数。\n",
    "\n",
    "BN：Batch Normalization，批量归一化，批是指一批数据，通常为 mini-batch；标准化是处理后的数据服从$N(0,1)$的正态分布。BN的思想是对每一层的输入做归一化，使得数据分布的均值为0，方差为1。公式为：\n",
    "\n",
    "$$y = \\gamma \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} + \\beta$$\n",
    "\n",
    "其中$y$是归一化后的输出，$\\gamma$和$\\beta$是可学习的系数，$\\mu_B$和$\\sigma_B^2$是批内均值和方差。\n",
    "\n",
    "GN和BN的区别：\n",
    "\n",
    "- GN的分组操作使得每组数据归一化，而BN是对整个批数据归一化。\n",
    "- GN的归一化公式中，$\\gamma$和$\\beta$是可学习的系数，可以适用于不同的输入。\n",
    "- BN的归一化公式中，$\\gamma$和$\\beta$是固定不变的，只能适用于特定输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ebae8",
   "metadata": {},
   "source": [
    "## <a id='toc1_13_'></a>[模型保存与加载](#toc0_)\n",
    "### <a id='toc1_13_1_'></a>[基本方法](#toc0_)\n",
    "#### <a id='toc1_13_1_1_'></a>[(1) 保存整个模型（结构+参数）](#toc0_)\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个简单模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(10, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model, 'model_entire.pth')  # 保存整个模型\n",
    "```\n",
    "\n",
    "#### <a id='toc1_13_1_2_'></a>[(2) 仅保存模型参数（推荐）](#toc0_)\n",
    "```python\n",
    "# 保存参数（体积更小）\n",
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "# 加载时需先重建结构\n",
    "new_model = SimpleModel()  # 必须与原模型结构相同\n",
    "new_model.load_state_dict(torch.load('model_weights.pth'))\n",
    "```\n",
    "\n",
    "### <a id='toc1_13_2_'></a>[带优化器的保存（用于恢复训练）](#toc0_)\n",
    "```python\n",
    "# 保存\n",
    "checkpoint = {\n",
    "    'epoch': 10,\n",
    "    'model_state': model.state_dict(),\n",
    "    'optimizer_state': optimizer.state_dict(),\n",
    "    'loss': loss_value\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n",
    "\n",
    "# 加载\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "epoch = checkpoint['epoch']\n",
    "```\n",
    "\n",
    "### <a id='toc1_13_3_'></a>[跨设备加载](#toc0_)\n",
    "```python\n",
    "# GPU保存 → CPU加载\n",
    "model = torch.load('gpu_model.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "# 自动适配设备\n",
    "model = SimpleModel()\n",
    "model.load_state_dict(torch.load('weights.pth', map_location='cuda:0' if torch.cuda.is_available() else 'cpu'))\n",
    "```\n",
    "\n",
    "### <a id='toc1_13_4_'></a>[注意事项](#toc0_)\n",
    "1. **文件扩展名**：建议使用 `.pth` 或 `.pt`  \n",
    "2. **版本兼容性**：\n",
    "   - 保存时指定PyTorch版本：`torch.save(..., _use_new_zipfile_serialization=True)`\n",
    "   - 加载旧模型可能需兼容模式：`torch.load(..., pickle_module=pickle)`\n",
    "3. **安全警告**：只加载可信来源的模型文件（可能包含恶意代码）\n",
    "\n",
    "### <a id='toc1_13_5_'></a>[完整示例代码](#toc0_)\n",
    "```python\n",
    "# 保存示例\n",
    "def save_model(model, path):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': {'input_dim': 10, 'output_dim': 2}  # 保存结构配置\n",
    "    }, path)\n",
    "\n",
    "# 加载示例\n",
    "def load_model(path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model = SimpleModel(**checkpoint['config'])  # 根据配置重建模型\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return model\n",
    "```\n",
    "\n",
    "### <a id='toc1_13_6_'></a>[常用文件结构](#toc0_)\n",
    "```\n",
    "project/\n",
    "├── models/\n",
    "│   ├── model_weights.pth    # 参数文件\n",
    "│   └── checkpoint.pth      # 训练状态\n",
    "└── train.py                # 训练/加载代码\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
